---
layout: archive
title: "Selected Papers"
permalink: /papers/
author_profile: true
---

{% include base_path %}

<font size="+2"><a href="https://arxiv.org/abs/2209.00588" style="color: black; text-decoration: None" onmouseover="style='color: black; text-decoration:underline'" onmouseout="style='color: black; text-decoration:none'">Transformers are Sample-Efficient World Models</a></font>  
Vincent Micheli, Eloi Alonso, François Fleuret  
*ICLR 2023 (**Oral**), NeurIPS Deep RL Workshop 2022 (**Best paper award**)*  
<a href="https://github.com/eloialonso/iris" style="text-decoration:None" onmouseover="style=' text-decoration:underline'" onmouseout="style='text-decoration:none'">Code</a> <a href="https://twitter.com/hardmaru/status/1565569808548376576" style="text-decoration:None" onmouseover="style=' text-decoration:underline'" onmouseout="style='text-decoration:none'">Hardmaru's tweet</a>

<font size="+2"><a href="https://arxiv.org/abs/2202.10583" style="color: black; text-decoration: None" onmouseover="style='color: black; text-decoration:underline'" onmouseout="style='color: black; text-decoration:none'">MineRL Diamond 2021 Competition: Overview, Results, and Lessons Learned</a></font>  
Anssi Kanervisto, Stephanie Milani, [...], Vincent Micheli, [...]  
*PMLR NeurIPS Competitions and Demonstrations Track 2022*  

<font size="+2"><a href="https://arxiv.org/abs/2104.07972" style="color: black; text-decoration: None" onmouseover="style='color: black; text-decoration:underline'" onmouseout="style='color: black; text-decoration:none'">Language Models are Few-Shot Butlers</a></font>  
Vincent Micheli, François Fleuret  
*EMNLP 2021*  
<a href="https://github.com/vmicheli/lm-butlers" style="text-decoration:None" onmouseover="style=' text-decoration:underline'" onmouseout="style='text-decoration:none'">Code</a>

<font size="+2"><a href="https://arxiv.org/abs/2010.03813" style="color: black; text-decoration: None" onmouseover="style='color: black; text-decoration:underline'" onmouseout="style='color: black; text-decoration:none'">On the importance of pre-training data volume for compact language models</a></font>  
Vincent Micheli, Martin d'Hoffschmidt, François Fleuret  
*EMNLP 2020*  

<font size="+2"><a href="https://arxiv.org/abs/2002.03240" style="color: black; text-decoration: None" onmouseover="style='color: black; text-decoration:underline'" onmouseout="style='color: black; text-decoration:none'">Multi-task Reinforcement Learning with a Planning Quasi-Metric</a></font>  
Vincent Micheli, Karthigan Sinnathamby, François Fleuret  
*NeurIPS Deep RL Workshop 2020*  